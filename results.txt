==========Baseline: Classification Report for LogsticRegression:==========
              precision    recall  f1-score   support

           0       0.60      0.10      0.18       477
           1       0.53      0.93      0.68       514

   micro avg       0.53      0.53      0.53       991
   macro avg       0.56      0.52      0.43       991
weighted avg       0.56      0.53      0.44       991

==========Baseline: Accuracy Report for LogsticRegression:==========
0.5348133198789102

==========Classification Report for LogsticRegression:==========
              precision    recall  f1-score   support

           0       0.52      0.52      0.52       473
           1       0.56      0.56      0.56       511

   micro avg       0.54      0.54      0.54       984
   macro avg       0.54      0.54      0.54       984
weighted avg       0.54      0.54      0.54       984

==========Accuracy Report for LogsticRegression:==========
0.540650406504065

2019-04-14 15:31:01.904723: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
Train on 2064 samples, validate on 230 samples
Epoch 1/16
2064/2064 [==============================] - 1s 431us/step - loss: 0.6964 - acc: 0.4898 - val_loss: 0.6975 - val_acc: 0.4652

Epoch 00001: val_loss improved from inf to 0.69754, saving model to ./weights/0.46522_0001.weights.h5
Epoch 2/16
2064/2064 [==============================] - 1s 282us/step - loss: 0.6902 - acc: 0.5378 - val_loss: 0.6965 - val_acc: 0.5522

Epoch 00002: val_loss improved from 0.69754 to 0.69646, saving model to ./weights/0.55217_0002.weights.h5
Epoch 3/16
2064/2064 [==============================] - 1s 279us/step - loss: 0.6802 - acc: 0.5630 - val_loss: 0.6971 - val_acc: 0.5435

Epoch 00003: val_loss did not improve from 0.69646
Epoch 4/16
2064/2064 [==============================] - 1s 284us/step - loss: 0.6587 - acc: 0.5930 - val_loss: 0.7002 - val_acc: 0.4957

Epoch 00004: val_loss did not improve from 0.69646
Epoch 5/16
2064/2064 [==============================] - 1s 325us/step - loss: 0.6278 - acc: 0.6492 - val_loss: 0.7653 - val_acc: 0.5174

Epoch 00005: val_loss did not improve from 0.69646
Epoch 00005: early stopping
==========Classification Report for Vanilla:==========
              precision    recall  f1-score   support

           0       0.49      0.75      0.59       473
           1       0.54      0.27      0.36       511

   micro avg       0.50      0.50      0.50       984
   macro avg       0.51      0.51      0.48       984
weighted avg       0.51      0.50      0.47       984

==========Accuracy Report for Vanilla:==========
0.5010162601626016

Train on 2064 samples, validate on 230 samples
Epoch 1/16
2064/2064 [==============================] - 10s 5ms/step - loss: 0.6942 - acc: 0.4729 - val_loss: 0.6897 - val_acc: 0.5261

Epoch 00001: val_loss improved from 0.69646 to 0.68970, saving model to ./weights/0.52609_0001.weights.h5
Epoch 2/16
2064/2064 [==============================] - 9s 5ms/step - loss: 0.6927 - acc: 0.5010 - val_loss: 0.6902 - val_acc: 0.5261

Epoch 00002: val_loss did not improve from 0.68970
Epoch 3/16
2064/2064 [==============================] - 9s 5ms/step - loss: 0.6918 - acc: 0.5107 - val_loss: 0.6881 - val_acc: 0.5391

Epoch 00003: val_loss improved from 0.68970 to 0.68809, saving model to ./weights/0.53913_0003.weights.h5
Epoch 4/16
2064/2064 [==============================] - 10s 5ms/step - loss: 0.6897 - acc: 0.5228 - val_loss: 0.6909 - val_acc: 0.5391

Epoch 00004: val_loss did not improve from 0.68809
Epoch 5/16
2064/2064 [==============================] - 9s 4ms/step - loss: 0.6891 - acc: 0.5325 - val_loss: 0.6856 - val_acc: 0.5261

Epoch 00005: val_loss improved from 0.68809 to 0.68561, saving model to ./weights/0.52609_0005.weights.h5
Epoch 6/16
2064/2064 [==============================] - 10s 5ms/step - loss: 0.6880 - acc: 0.5412 - val_loss: 0.6851 - val_acc: 0.5348

Epoch 00006: val_loss improved from 0.68561 to 0.68507, saving model to ./weights/0.53478_0006.weights.h5
Epoch 00006: early stopping
==========Classification Report for GRU:==========
              precision    recall  f1-score   support

           0       0.57      0.36      0.44       473
           1       0.56      0.74      0.64       511

   micro avg       0.56      0.56      0.56       984
   macro avg       0.56      0.55      0.54       984
weighted avg       0.56      0.56      0.54       984

==========Accuracy Report for GRU:==========
0.5599593495934959

Train on 2064 samples, validate on 230 samples
Epoch 1/16
2064/2064 [==============================] - 13s 6ms/step - loss: 0.6932 - acc: 0.5150 - val_loss: 0.6908 - val_acc: 0.5304

Epoch 00001: val_loss did not improve from 0.68507
Epoch 2/16
2064/2064 [==============================] - 11s 6ms/step - loss: 0.6919 - acc: 0.5160 - val_loss: 0.6929 - val_acc: 0.5174

Epoch 00002: val_loss did not improve from 0.68507
Epoch 3/16
2064/2064 [==============================] - 11s 5ms/step - loss: 0.6913 - acc: 0.5141 - val_loss: 0.6882 - val_acc: 0.5522

Epoch 00003: val_loss did not improve from 0.68507
Epoch 4/16
2064/2064 [==============================] - 11s 5ms/step - loss: 0.6888 - acc: 0.5441 - val_loss: 0.6861 - val_acc: 0.5391

Epoch 00004: val_loss did not improve from 0.68507
Epoch 5/16
2064/2064 [==============================] - 11s 5ms/step - loss: 0.6881 - acc: 0.5359 - val_loss: 0.6892 - val_acc: 0.5130

Epoch 00005: val_loss did not improve from 0.68507
Epoch 6/16
2064/2064 [==============================] - 11s 5ms/step - loss: 0.6862 - acc: 0.5431 - val_loss: 0.6857 - val_acc: 0.5217

Epoch 00006: val_loss did not improve from 0.68507
Epoch 00006: early stopping
==========Classification Report for LSTM:==========
              precision    recall  f1-score   support

           0       0.49      0.32      0.38       473
           1       0.52      0.70      0.60       511

   micro avg       0.52      0.52      0.52       984
   macro avg       0.51      0.51      0.49       984
weighted avg       0.51      0.52      0.50       984

==========Accuracy Report for LSTM:==========
0.5152439024390244

==========Classification Report for RandomForest:==========
              precision    recall  f1-score   support

           0       0.52      0.45      0.48       473
           1       0.55      0.61      0.58       511

   micro avg       0.53      0.53      0.53       984
   macro avg       0.53      0.53      0.53       984
weighted avg       0.53      0.53      0.53       984

==========Accuracy Report for RandomForest:==========
0.5345528455284553

==========Classification Report for XGBoost:==========
              precision    recall  f1-score   support

           0       0.51      0.49      0.50       473
           1       0.54      0.55      0.55       511

   micro avg       0.52      0.52      0.52       984
   macro avg       0.52      0.52      0.52       984
weighted avg       0.52      0.52      0.52       984

==========Accuracy Report for XGBoost:==========
0.524390243902439

[LightGBM] [Warning] Starting from the 2.1.2 version, default value for the "boost_from_average" parameter in "binary" objective is true.
This may cause significantly different results comparing to the previous versions of LightGBM.
Try to set boost_from_average=false, if your old models produce bad results
[LightGBM] [Info] Number of positive: 1051, number of negative: 1013
[LightGBM] [Info] Total Bins 122900
[LightGBM] [Info] Number of data: 2064, number of used features: 1700
[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.509205 -> initscore=0.036826
[LightGBM] [Info] Start training from score 0.036826
[1] train's binary_error: 0.318798  eval's binary_error: 0.543478
Training until validation scores don't improve for 10 rounds.
[2] train's binary_error: 0.255329  eval's binary_error: 0.552174
[3] train's binary_error: 0.207849  eval's binary_error: 0.530435
[4] train's binary_error: 0.179264  eval's binary_error: 0.534783
[5] train's binary_error: 0.157461  eval's binary_error: 0.526087
[6] train's binary_error: 0.140019  eval's binary_error: 0.552174
[7] train's binary_error: 0.122578  eval's binary_error: 0.565217
[8] train's binary_error: 0.109981  eval's binary_error: 0.530435
[9] train's binary_error: 0.0978682 eval's binary_error: 0.552174
[10]  train's binary_error: 0.0852713 eval's binary_error: 0.53913
[11]  train's binary_error: 0.0760659 eval's binary_error: 0.543478
[12]  train's binary_error: 0.063469  eval's binary_error: 0.543478
[13]  train's binary_error: 0.0557171 eval's binary_error: 0.56087
[14]  train's binary_error: 0.0513566 eval's binary_error: 0.53913
[15]  train's binary_error: 0.0411822 eval's binary_error: 0.547826
Early stopping, best iteration is:
[5] train's binary_error: 0.157461  eval's binary_error: 0.526087
==========Classification Report for LGBM:==========
              precision    recall  f1-score   support

           0       0.51      0.47      0.49       473
           1       0.54      0.58      0.56       511

   micro avg       0.53      0.53      0.53       984
   macro avg       0.53      0.53      0.53       984
weighted avg       0.53      0.53      0.53       984

==========Accuracy Report for LGBM:==========
0.5304878048780488

==========Classification Report for Ensemble:==========
              precision    recall  f1-score   support

           0       0.55      0.53      0.54       473
           1       0.58      0.60      0.59       511

   micro avg       0.57      0.57      0.57       984
   macro avg       0.56      0.56      0.56       984
weighted avg       0.57      0.57      0.57       984

==========Accuracy Report for Ensemble:==========
0.5660569105691057
