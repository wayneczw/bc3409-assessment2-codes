Epoch 1/16
2180/2180 [==============================] - 0s 164us/step - loss: 0.6957 - acc: 0.5225 - val_loss: 0.6974 - val_acc: 0.5021

Epoch 00001: val_loss improved from inf to 0.69745, saving model to ./weights/0.69745_0001.weights.h5
Epoch 2/16
2180/2180 [==============================] - 0s 40us/step - loss: 0.6843 - acc: 0.5642 - val_loss: 0.6989 - val_acc: 0.4733

Epoch 00002: val_loss did not improve from 0.69745
Epoch 3/16
2180/2180 [==============================] - 0s 40us/step - loss: 0.6702 - acc: 0.6087 - val_loss: 0.7046 - val_acc: 0.4609

Epoch 00003: val_loss did not improve from 0.69745
Epoch 4/16
2180/2180 [==============================] - 0s 40us/step - loss: 0.6478 - acc: 0.6339 - val_loss: 0.7129 - val_acc: 0.5185

Epoch 00004: val_loss did not improve from 0.69745
Epoch 00004: early stopping
==========Classification Report for Vanilla:==========
              precision    recall  f1-score   support

         0.0       0.49      0.57      0.53       505
         1.0       0.52      0.45      0.48       534

   micro avg       0.51      0.51      0.51      1039
   macro avg       0.51      0.51      0.51      1039
weighted avg       0.51      0.51      0.50      1039

==========Accuracy Report for Vanilla:==========
0.5062560153994226

Train on 2180 samples, validate on 243 samples
Epoch 1/16
2180/2180 [==============================] - 2s 1ms/step - loss: 0.6970 - acc: 0.4931 - val_loss: 0.6937 - val_acc: 0.4856

Epoch 00001: val_loss improved from 0.69745 to 0.69370, saving model to ./weights/0.69370_0001.weights.h5
Epoch 2/16
2180/2180 [==============================] - 1s 561us/step - loss: 0.6932 - acc: 0.5119 - val_loss: 0.6946 - val_acc: 0.4527

Epoch 00002: val_loss did not improve from 0.69370
Epoch 3/16
2180/2180 [==============================] - 1s 538us/step - loss: 0.6929 - acc: 0.5197 - val_loss: 0.6930 - val_acc: 0.5391

Epoch 00003: val_loss improved from 0.69370 to 0.69298, saving model to ./weights/0.69298_0003.weights.h5
Epoch 4/16
2180/2180 [==============================] - 1s 527us/step - loss: 0.6924 - acc: 0.5188 - val_loss: 0.6945 - val_acc: 0.4856

Epoch 00004: val_loss did not improve from 0.69298
Epoch 00004: early stopping
==========Classification Report for GRU:==========
              precision    recall  f1-score   support

         0.0       0.46      0.53      0.49       505
         1.0       0.48      0.41      0.44       534

   micro avg       0.47      0.47      0.47      1039
   macro avg       0.47      0.47      0.47      1039
weighted avg       0.47      0.47      0.47      1039

==========Accuracy Report for GRU:==========
0.46775745909528393

Train on 2180 samples, validate on 243 samples
Epoch 1/16
2180/2180 [==============================] - 3s 1ms/step - loss: 0.6934 - acc: 0.5188 - val_loss: 0.6937 - val_acc: 0.5185

Epoch 00001: val_loss did not improve from 0.69298
Epoch 2/16
2180/2180 [==============================] - 1s 632us/step - loss: 0.6929 - acc: 0.5193 - val_loss: 0.6947 - val_acc: 0.5185

Epoch 00002: val_loss did not improve from 0.69298
Epoch 3/16
2180/2180 [==============================] - 1s 633us/step - loss: 0.6929 - acc: 0.5142 - val_loss: 0.6962 - val_acc: 0.5103

Epoch 00003: val_loss did not improve from 0.69298
Epoch 4/16
2180/2180 [==============================] - 1s 642us/step - loss: 0.6924 - acc: 0.5170 - val_loss: 0.6946 - val_acc: 0.4897

Epoch 00004: val_loss did not improve from 0.69298
Epoch 00004: early stopping
==========Classification Report for LSTM:==========
              precision    recall  f1-score   support

         0.0       0.47      0.71      0.56       505
         1.0       0.46      0.23      0.30       534

   micro avg       0.46      0.46      0.46      1039
   macro avg       0.46      0.47      0.43      1039
weighted avg       0.46      0.46      0.43      1039

==========Accuracy Report for LSTM:==========
0.46487006737247355

==========Classification Report for XGBoost:==========
              precision    recall  f1-score   support

         0.0       0.70      0.72      0.71       505
         1.0       0.73      0.71      0.72       534

   micro avg       0.72      0.72      0.72      1039
   macro avg       0.72      0.72      0.72      1039
weighted avg       0.72      0.72      0.72      1039

==========Accuracy Report for XGBoost:==========
0.7179980750721848

[LightGBM] [Warning] Starting from the 2.1.2 version, default value for the "boost_from_average" parameter in "binary" objective is true.
This may cause significantly different results comparing to the previous versions of LightGBM.
Try to set boost_from_average=false, if your old models produce bad results
[LightGBM] [Info] Number of positive: 1044, number of negative: 1136
[LightGBM] [Info] Total Bins 71675
[LightGBM] [Info] Number of data: 2180, number of used features: 1475
[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.478899 -> initscore=-0.084454
[LightGBM] [Info] Start training from score -0.084454
[1]	train's binary_error: 0.205046	eval's binary_error: 0.366255
Training until validation scores don't improve for 10 rounds.
[2]	train's binary_error: 0.165138	eval's binary_error: 0.312757
[3]	train's binary_error: 0.151835	eval's binary_error: 0.300412
[4]	train's binary_error: 0.139908	eval's binary_error: 0.296296
[5]	train's binary_error: 0.127523	eval's binary_error: 0.316872
[6]	train's binary_error: 0.122477	eval's binary_error: 0.312757
[7]	train's binary_error: 0.113761	eval's binary_error: 0.325103
[8]	train's binary_error: 0.105963	eval's binary_error: 0.312757
[9]	train's binary_error: 0.105505	eval's binary_error: 0.312757
[10]	train's binary_error: 0.0995413	eval's binary_error: 0.316872
[11]	train's binary_error: 0.0995413	eval's binary_error: 0.312757
[12]	train's binary_error: 0.0958716	eval's binary_error: 0.320988
[13]	train's binary_error: 0.0922018	eval's binary_error: 0.316872
[14]	train's binary_error: 0.0931193	eval's binary_error: 0.300412
Early stopping, best iteration is:
[4]	train's binary_error: 0.139908	eval's binary_error: 0.296296
==========Classification Report for LGBM:==========
              precision    recall  f1-score   support

         0.0       0.69      0.74      0.71       505
         1.0       0.74      0.69      0.71       534

   micro avg       0.71      0.71      0.71      1039
   macro avg       0.71      0.71      0.71      1039
weighted avg       0.71      0.71      0.71      1039

==========Accuracy Report for LGBM:==========
0.7122232916265641
