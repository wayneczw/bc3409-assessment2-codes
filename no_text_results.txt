==========Classification Report for LogsticRegression:==========
              precision    recall  f1-score   support

         0.0       0.57      0.59      0.58       505
         1.0       0.60      0.58      0.59       534

   micro avg       0.59      0.59      0.59      1039
   macro avg       0.59      0.59      0.59      1039
weighted avg       0.59      0.59      0.59      1039

==========Accuracy Report for LogsticRegression:==========
0.5861405197305101


Train on 2180 samples, validate on 243 samples
Epoch 1/16
2180/2180 [==============================] - 0s 210us/step - loss: 0.6948 - acc: 0.5197 - val_loss: 0.6949 - val_acc: 0.4733

Epoch 00001: val_loss improved from inf to 0.69493, saving model to ./weights/0.69493_0001.weights.h5
Epoch 2/16
2180/2180 [==============================] - 0s 60us/step - loss: 0.6807 - acc: 0.5766 - val_loss: 0.6972 - val_acc: 0.4774

Epoch 00002: val_loss did not improve from 0.69493
Epoch 3/16
2180/2180 [==============================] - 0s 56us/step - loss: 0.6662 - acc: 0.6234 - val_loss: 0.6988 - val_acc: 0.4938

Epoch 00003: val_loss did not improve from 0.69493
Epoch 4/16
2180/2180 [==============================] - 0s 56us/step - loss: 0.6462 - acc: 0.6596 - val_loss: 0.7084 - val_acc: 0.5062

Epoch 00004: val_loss did not improve from 0.69493
Epoch 00004: early stopping
==========Classification Report for Vanilla:==========
              precision    recall  f1-score   support

         0.0       0.49      0.67      0.56       505
         1.0       0.51      0.33      0.40       534

   micro avg       0.49      0.49      0.49      1039
   macro avg       0.50      0.50      0.48      1039
weighted avg       0.50      0.49      0.48      1039

==========Accuracy Report for Vanilla:==========
0.49470644850818096

Train on 2180 samples, validate on 243 samples
Epoch 1/16
2180/2180 [==============================] - 3s 1ms/step - loss: 0.6952 - acc: 0.5014 - val_loss: 0.6942 - val_acc: 0.5226

Epoch 00001: val_loss improved from 0.69493 to 0.69419, saving model to ./weights/0.69419_0001.weights.h5
Epoch 2/16
2180/2180 [==============================] - 1s 598us/step - loss: 0.6930 - acc: 0.5151 - val_loss: 0.6944 - val_acc: 0.4856

Epoch 00002: val_loss did not improve from 0.69419
Epoch 3/16
2180/2180 [==============================] - 1s 571us/step - loss: 0.6926 - acc: 0.5083 - val_loss: 0.6943 - val_acc: 0.4815

Epoch 00003: val_loss did not improve from 0.69419
Epoch 4/16
2180/2180 [==============================] - 1s 638us/step - loss: 0.6924 - acc: 0.5161 - val_loss: 0.6950 - val_acc: 0.4733

Epoch 00004: val_loss did not improve from 0.69419
Epoch 00004: early stopping
==========Classification Report for GRU:==========
              precision    recall  f1-score   support

         0.0       0.46      0.38      0.41       505
         1.0       0.49      0.58      0.53       534

   micro avg       0.48      0.48      0.48      1039
   macro avg       0.48      0.48      0.47      1039
weighted avg       0.48      0.48      0.47      1039

==========Accuracy Report for GRU:==========
0.4793070259865255

Train on 2180 samples, validate on 243 samples
Epoch 1/16
2180/2180 [==============================] - 3s 1ms/step - loss: 0.6940 - acc: 0.5156 - val_loss: 0.6929 - val_acc: 0.5226

Epoch 00001: val_loss improved from 0.69419 to 0.69292, saving model to ./weights/0.69292_0001.weights.h5
Epoch 2/16
2180/2180 [==============================] - 2s 794us/step - loss: 0.6932 - acc: 0.5110 - val_loss: 0.6930 - val_acc: 0.5144

Epoch 00002: val_loss did not improve from 0.69292
Epoch 3/16
2180/2180 [==============================] - 2s 872us/step - loss: 0.6931 - acc: 0.5165 - val_loss: 0.6926 - val_acc: 0.5185

Epoch 00003: val_loss improved from 0.69292 to 0.69257, saving model to ./weights/0.69257_0003.weights.h5
Epoch 4/16
2180/2180 [==============================] - 2s 817us/step - loss: 0.6929 - acc: 0.5170 - val_loss: 0.6931 - val_acc: 0.5144

Epoch 00004: val_loss did not improve from 0.69257
Epoch 00004: early stopping
==========Classification Report for LSTM:==========
              precision    recall  f1-score   support

         0.0       0.48      0.68      0.56       505
         1.0       0.50      0.30      0.37       534

   micro avg       0.48      0.48      0.48      1039
   macro avg       0.49      0.49      0.47      1039
weighted avg       0.49      0.48      0.46      1039

==========Accuracy Report for LSTM:==========
0.48411934552454283

==========Classification Report for RandomForest:==========
              precision    recall  f1-score   support

         0.0       0.69      0.75      0.72       505
         1.0       0.74      0.68      0.71       534

   micro avg       0.71      0.71      0.71      1039
   macro avg       0.72      0.72      0.71      1039
weighted avg       0.72      0.71      0.71      1039

==========Accuracy Report for RandomForest:==========
0.714148219441771

==========Classification Report for XGBoost:==========
              precision    recall  f1-score   support

         0.0       0.72      0.70      0.71       505
         1.0       0.73      0.75      0.74       534

   micro avg       0.73      0.73      0.73      1039
   macro avg       0.73      0.73      0.73      1039
weighted avg       0.73      0.73      0.73      1039

==========Accuracy Report for XGBoost:==========
0.7256977863330125

[LightGBM] [Warning] Starting from the 2.1.2 version, default value for the "boost_from_average" parameter in "binary" objective is true.
This may cause significantly different results comparing to the previous versions of LightGBM.
Try to set boost_from_average=false, if your old models produce bad results
[LightGBM] [Info] Number of positive: 1044, number of negative: 1136
[LightGBM] [Info] Total Bins 39675
[LightGBM] [Info] Number of data: 2180, number of used features: 1350
[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.478899 -> initscore=-0.084454
[LightGBM] [Info] Start training from score -0.084454
[1] train's binary_error: 0.201835  eval's binary_error: 0.341564
Training until validation scores don't improve for 10 rounds.
[2] train's binary_error: 0.173394  eval's binary_error: 0.329218
[3] train's binary_error: 0.150459  eval's binary_error: 0.304527
[4] train's binary_error: 0.143119  eval's binary_error: 0.316872
[5] train's binary_error: 0.129358  eval's binary_error: 0.320988
[6] train's binary_error: 0.120183  eval's binary_error: 0.304527
[7] train's binary_error: 0.123853  eval's binary_error: 0.308642
[8] train's binary_error: 0.115138  eval's binary_error: 0.312757
[9] train's binary_error: 0.11055 eval's binary_error: 0.312757
[10]  train's binary_error: 0.10367 eval's binary_error: 0.312757
[11]  train's binary_error: 0.0972477 eval's binary_error: 0.296296
[12]  train's binary_error: 0.0954128 eval's binary_error: 0.300412
[13]  train's binary_error: 0.0940367 eval's binary_error: 0.304527
[14]  train's binary_error: 0.090367  eval's binary_error: 0.304527
[15]  train's binary_error: 0.0866972 eval's binary_error: 0.308642
[16]  train's binary_error: 0.0876147 eval's binary_error: 0.300412
[17]  train's binary_error: 0.0816514 eval's binary_error: 0.308642
[18]  train's binary_error: 0.0788991 eval's binary_error: 0.304527
[19]  train's binary_error: 0.0761468 eval's binary_error: 0.308642
[20]  train's binary_error: 0.0720183 eval's binary_error: 0.312757
[21]  train's binary_error: 0.0697248 eval's binary_error: 0.308642
Early stopping, best iteration is:
[11]  train's binary_error: 0.0972477 eval's binary_error: 0.296296
==========Classification Report for LGBM:==========
              precision    recall  f1-score   support

         0.0       0.70      0.73      0.71       505
         1.0       0.73      0.70      0.72       534

   micro avg       0.72      0.72      0.72      1039
   macro avg       0.72      0.72      0.72      1039
weighted avg       0.72      0.72      0.72      1039

==========Accuracy Report for LGBM:==========
0.7160731472569779

==========Classification Report for Ensemble:==========
              precision    recall  f1-score   support

         0.0       0.72      0.75      0.74       505
         1.0       0.76      0.72      0.74       534

   micro avg       0.74      0.74      0.74      1039
   macro avg       0.74      0.74      0.74      1039
weighted avg       0.74      0.74      0.74      1039

==========Accuracy Report for Ensemble:==========
0.737247353224254
