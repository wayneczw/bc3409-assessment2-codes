==========Classification Report for LogsticRegression:==========
              precision    recall  f1-score   support

           0       0.52      0.52      0.52       473
           1       0.55      0.56      0.56       511

   micro avg       0.54      0.54      0.54       984
   macro avg       0.54      0.54      0.54       984
weighted avg       0.54      0.54      0.54       984

==========Accuracy Report for LogsticRegression:==========
0.5376016260162602

2019-04-14 15:50:10.776229: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
Train on 2064 samples, validate on 230 samples
Epoch 1/16
2064/2064 [==============================] - 1s 422us/step - loss: 0.6955 - acc: 0.4927 - val_loss: 0.6897 - val_acc: 0.5522

Epoch 00001: val_loss improved from inf to 0.68966, saving model to ./weights/0.55217_0001.weights.h5
Epoch 2/16
2064/2064 [==============================] - 1s 286us/step - loss: 0.6840 - acc: 0.5557 - val_loss: 0.6834 - val_acc: 0.5522

Epoch 00002: val_loss improved from 0.68966 to 0.68341, saving model to ./weights/0.55217_0002.weights.h5
Epoch 3/16
2064/2064 [==============================] - 1s 277us/step - loss: 0.6675 - acc: 0.5872 - val_loss: 0.6897 - val_acc: 0.5391

Epoch 00003: val_loss did not improve from 0.68341
Epoch 4/16
2064/2064 [==============================] - 1s 272us/step - loss: 0.6328 - acc: 0.6458 - val_loss: 0.7038 - val_acc: 0.5435

Epoch 00004: val_loss did not improve from 0.68341
Epoch 00004: early stopping
==========Classification Report for Vanilla:==========
              precision    recall  f1-score   support

           0       0.50      0.55      0.52       473
           1       0.54      0.50      0.52       511

   micro avg       0.52      0.52      0.52       984
   macro avg       0.52      0.52      0.52       984
weighted avg       0.52      0.52      0.52       984

==========Accuracy Report for Vanilla:==========
0.5223577235772358

Train on 2064 samples, validate on 230 samples
Epoch 1/16
2064/2064 [==============================] - 10s 5ms/step - loss: 0.6946 - acc: 0.4816 - val_loss: 0.6936 - val_acc: 0.5348

Epoch 00001: val_loss did not improve from 0.68341
Epoch 2/16
2064/2064 [==============================] - 9s 4ms/step - loss: 0.6929 - acc: 0.5145 - val_loss: 0.6947 - val_acc: 0.4957

Epoch 00002: val_loss did not improve from 0.68341
Epoch 3/16
2064/2064 [==============================] - 9s 4ms/step - loss: 0.6928 - acc: 0.5203 - val_loss: 0.6925 - val_acc: 0.5087

Epoch 00003: val_loss did not improve from 0.68341
Epoch 4/16
2064/2064 [==============================] - 9s 5ms/step - loss: 0.6926 - acc: 0.5165 - val_loss: 0.6924 - val_acc: 0.5261

Epoch 00004: val_loss did not improve from 0.68341
Epoch 00004: early stopping
==========Classification Report for GRU:==========
              precision    recall  f1-score   support

           0       0.50      0.64      0.56       473
           1       0.55      0.41      0.47       511

   micro avg       0.52      0.52      0.52       984
   macro avg       0.53      0.52      0.51       984
weighted avg       0.53      0.52      0.51       984

==========Accuracy Report for GRU:==========
0.5193089430894309

Train on 2064 samples, validate on 230 samples
Epoch 1/16
2064/2064 [==============================] - 13s 6ms/step - loss: 0.6931 - acc: 0.5015 - val_loss: 0.6940 - val_acc: 0.4870

Epoch 00001: val_loss did not improve from 0.68341
Epoch 2/16
2064/2064 [==============================] - 11s 5ms/step - loss: 0.6931 - acc: 0.4913 - val_loss: 0.6941 - val_acc: 0.4870

Epoch 00002: val_loss did not improve from 0.68341
Epoch 3/16
2064/2064 [==============================] - 11s 5ms/step - loss: 0.6931 - acc: 0.4971 - val_loss: 0.6941 - val_acc: 0.4870

Epoch 00003: val_loss did not improve from 0.68341
Epoch 4/16
2064/2064 [==============================] - 11s 5ms/step - loss: 0.6931 - acc: 0.5145 - val_loss: 0.6941 - val_acc: 0.4870

Epoch 00004: val_loss did not improve from 0.68341
Epoch 00004: early stopping
==========Classification Report for LSTM:==========
/Users/wayneczw/ntu-projects/bc3409-assessment2-codes/venv/lib/python3.5/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)
              precision    recall  f1-score   support

           0       0.00      0.00      0.00       473
           1       0.52      1.00      0.68       511

   micro avg       0.52      0.52      0.52       984
   macro avg       0.26      0.50      0.34       984
weighted avg       0.27      0.52      0.36       984

==========Accuracy Report for LSTM:==========
0.5193089430894309

==========Classification Report for RandomForest:==========
              precision    recall  f1-score   support

           0       0.48      0.42      0.45       473
           1       0.52      0.57      0.54       511

   micro avg       0.50      0.50      0.50       984
   macro avg       0.50      0.50      0.50       984
weighted avg       0.50      0.50      0.50       984

==========Accuracy Report for RandomForest:==========
0.5

==========Classification Report for XGBoost:==========
              precision    recall  f1-score   support

           0       0.50      0.50      0.50       473
           1       0.54      0.55      0.54       511

   micro avg       0.52      0.52      0.52       984
   macro avg       0.52      0.52      0.52       984
weighted avg       0.52      0.52      0.52       984

==========Accuracy Report for XGBoost:==========
0.5223577235772358

[LightGBM] [Warning] Starting from the 2.1.2 version, default value for the "boost_from_average" parameter in "binary" objective is true.
This may cause significantly different results comparing to the previous versions of LightGBM.
Try to set boost_from_average=false, if your old models produce bad results
[LightGBM] [Info] Number of positive: 1051, number of negative: 1013
[LightGBM] [Info] Total Bins 110125
[LightGBM] [Info] Number of data: 2064, number of used features: 1650
[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.509205 -> initscore=0.036826
[LightGBM] [Info] Start training from score 0.036826
[1] train's binary_error: 0.322674  eval's binary_error: 0.530435
Training until validation scores don't improve for 10 rounds.
[2] train's binary_error: 0.267442  eval's binary_error: 0.552174
[3] train's binary_error: 0.234496  eval's binary_error: 0.534783
[4] train's binary_error: 0.205911  eval's binary_error: 0.565217
[5] train's binary_error: 0.176357  eval's binary_error: 0.556522
[6] train's binary_error: 0.157946  eval's binary_error: 0.552174
[7] train's binary_error: 0.146318  eval's binary_error: 0.552174
[8] train's binary_error: 0.135174  eval's binary_error: 0.56087
[9] train's binary_error: 0.127422  eval's binary_error: 0.56087
[10]  train's binary_error: 0.109496  eval's binary_error: 0.56087
[11]  train's binary_error: 0.0949612 eval's binary_error: 0.56087
Early stopping, best iteration is:
[1] train's binary_error: 0.322674  eval's binary_error: 0.530435
==========Classification Report for LGBM:==========
              precision    recall  f1-score   support

           0       0.54      0.32      0.40       473
           1       0.54      0.75      0.63       511

   micro avg       0.54      0.54      0.54       984
   macro avg       0.54      0.54      0.52       984
weighted avg       0.54      0.54      0.52       984

==========Accuracy Report for LGBM:==========
0.5436991869918699

==========Classification Report for Ensemble:==========
              precision    recall  f1-score   support

           0       0.53      0.48      0.50       473
           1       0.56      0.61      0.58       511

   micro avg       0.54      0.54      0.54       984
   macro avg       0.54      0.54      0.54       984
weighted avg       0.54      0.54      0.54       984

==========Accuracy Report for Ensemble:==========
0.5447154471544715